---
title: "Final"
author: "Alexa Andrade"
date: 6/08/25
editor: visual

execute:
  warning: false
  error: false
  message: false

format:
  html:
    toc: true
    toc-depth: 5
---

### link to Github repo:https://github.com/Alex2002UC/ENVS-193DS_spring-2025_final

```{r set up}
#reading in necessary packages 
library(tidyverse)   # general use 
library(here)        # For getting into file paths 


library(readxl)      # For reading in Excel files
library(janitor)     # For cleaning column names and tabulating-clean_names()


library(flextable)   #For tables
library(gt)   
library(ggplot2) #for plots 


library(scales) 
library(ggeffects) 
library(MuMIn) 
library(gtsummary)
library(DHARMa)

sst <- read_csv(here("Data", "SST_update2023.csv")) #Read in datsets 
nest_boxes <- read_csv(here("Data", "occdist.csv"))  

```


## Problem 1. Research writing (36 points)

### Problem

#### a. Transparent statistical methods (8 points)

In part 1, my co-worker used a Pearson's Correlation (r) Test, to measure the linear relationship between distance to head water(km) and annual total nitrogen load(kg year-1) In part 2, they used a one-way ANOVA, to test whether the mean nitrogen load would change among different sources: urban land, atmospheric deposition, fertilizer, waste water treatment, and grasslands.

#### b. More information needed (8 points)

My coworker should include the effect size and complete a Tukey's Honestly significant difference test to add more context and clarity to the results. Including the effect size will determine how large of an effect the nitrogen sources have on annual nitrogen load, and how much these predictor variables can explain the response variable, annual nitrogen load. While the p-value tells us that annual nitrogen load differs among different nitrogen sources, it doesn't tell us how much of the variation in anuual nitrogen load can be explained by the type of nitrogen source. The effect size adds context by describing how important the type of nitrogen source is when looking at annual nitrogen loads. While the One-way Anova tells us whether there is a difference between nitrogen sources and annual nitrogen loads, A Tukey's HSD will tell us which specific sources are different from eachother. It would identify which nitrogen sources: urban land, atmospheric deposition, fertilizer, waste water treatment, and grassland, hae different nitrogen loads. This woudl add context by helping people identify which nitrogen sources contribute more, and understand more about the environmental problem as a whole.

#### c. Suggestions for rewriting (20 points)

In part 1 of the results, your coworker has written:

> We rejected the null hypothesis that there is no correlation between distance from headwater (km) and annual total nitrogen load (kg year^-1^) (p = 0.03).

We found a (strong/weak/positive/negative-Here insert the effect size)(Pearson’s r = correlation coefficient) relationship between distance from headwater(km) and annual total nitrogen load(kg year^-1^) ( p = 0.03, α = significance level), where distance from headwater(km) significatly predicted annual nitrogen load nitrogen load(kg year^-1^).

In part 2 of the results section of the report, your co-worker has written:

> We rejected the null hypothesis that there is no difference in average nitrogen load (kg year^-1^) between sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands) (p = 0.02).

We found that there is a (Insert effect size) difference between average nitrogen load (kg year^-1^) between sources: urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands(one-way ANOVA, F(among groups df, within groups df) = F-statistic, p = 0.02, \$\alpha = significance level).

## Problem 2. Data visualization (36 points)

### Skills you will demonstrate

### Problem

#### a. Cleaning and summarizing (16 points)

Create an object called `sst_clean` from `sst`. Clean and summarize the data such that you end up with a data frame like this:

```{r clean data}
sst_clean <- sst |> 
  clean_names() |> #standarizes column names 
    mutate( 
    date = ymd(date),     #changing date column to date format          
    year = year(date),    #takes year values from date column         
    month = month(date)) |> #takes month values from date
  
  filter(year == c("2018", "2019", "2020", "2021", "2022", "2023")) |> #filtering for only these yeaes
   select(year, month, temp) |> #selecting only these columns 
   group_by(year, month) |> #grouping by year and month
  summarise(mean_monthly_sst = round((mean(temp, na.rm = TRUE)), 1)) |>  #calculating mean temp per year and month group, also roudning to the first decimal point
    mutate(year = as.factor(year)) |> #set year column as a factor
    mutate(month = factor(month.abb[month], #set the month column to factor 
                           levels = month.abb, #the levels of the ordinal factor are set my the month abbreviations
                           ordered = TRUE)) |> #months are an ordered factor 
  ungroup() 

slice_sample(sst_clean, n = 5)  #display 5 random rows from sst_clean
str(sst_clean) #display sst_clean data structure
```

#### b. Visualize the data (20 points)

Recreate this visualization:

```{r Visualization}

ggplot(sst_clean, aes(x = month, #setting up x axis as month
                      y = mean_monthly_sst, #plotting mean temperature per month on y-axis
                      color = year, #different colors for each year
                      group = year)) + #tells the plot to draw lines by connecting the dots for each year
geom_line(size = 1) + # Adds lines to show monthly trends for each water year
geom_point() + # Add points to represent underlying data
labs(
x = "Month", #x-axis name
y = "Mean Monthly Sea Surface temperature (°C)", #y-axis name
color = "Year") + #differentiates each water year by a different color
    scale_color_manual( #manually assighning colors to each year)
    values = c( 
      "2018" = "#c6dbef",
      "2019" = "#9ecae1",
      "2020" = "#6baed6",
      "2021" = "#4292c6",
      "2022" = "#2171b5",
      "2023" = "#084594")) + #made a blue color gradient 

theme_minimal(base_size = 14) + #changing theme from default 
theme( 
panel.background = element_blank(), #gets rid of backround panels 
panel.grid = element_blank(), #no grid
    axis.ticks = element_line(color = "black"), #adds axis ticks 
    axis.line = element_line(color = "black"), #makes axis lines black 
plot.title = element_text(hjust = 0),
legend.position = c(0.1, 0.7), #posiitons legend inside graph
legend.background = element_blank() #gets rid of backround box for legend 
)

```

## Problem 3. Data analysis (87 points)

### Skills you will demonstrate

1.  How do year (2016 or 2019) and distance from forest edge predict Swift Parrot (*Lathamus discolor*) nest box occupancy?
2.  Is there a simpler model that explains Swift Parrot nest box occupancy, and if so, what is it?

**READ THE INTRODUCTION AND METHODS BEFORE YOU START.**

### Problem

#### a. Response variable (2 points)

In 1-2 sentences, explain what the 1s and 0s mean in this data set biologically.

The 0s in this data mean that a nest box is unoccupied by a certain species: common starling, swift parrot, or tree martin. The 1s in this data mean that a nest box is occupied by whatever species is labelled in that row.

#### b. Purpose of study (2 points)

The authors compare nest box occupancy between 3 species: Swift Parrots, Common Starlings, and Tree Martins. In 1-2 sentences, explain the main difference between Swift Parrots and the other two species in the context of this study.

Swift Parrots are an native species of birds that are endangered due to deforestation and invasive species. In comparison, Common Starlings are an abundant introduced species and the Tree Martins are a native species that is not endangered, however, both of these species are nest box competitors for the Swift Parrots.

#### c. Difference in "seasons" (2 points)

The authors compare two years (that they refer to as "seasons"). In 1-2 sentences, define what those years/seasons are, and explain how they differ in the context of this study.

The two years that are referred to as breeding seasons in thi study are 2016 and 2019. In 2016, the nest boxes were newly deployed and 29 were used by Swift Parrots, in comparison, in 2019, the nest boxes had been around for three years and only 20 were used, with only five nests reused.

#### d. Table of models (10 points)

Make a table of all the models you will need to run. You will run 4 models: a null model, a saturated model, and two other models with different combinations of predictors.

Stuck on how to create a table? See workshop 8 for an example.

Your table should have 4 columns: (1) model number, (2) season, (3) distance to forest edge, and (4) model description.

```{r CLean and Process Data}
#| echo: false
#| results: "hide"
#| message: false
#| warning: false

nest_boxes_clean <- nest_boxes |> #new object
  clean_names() |> #standarized column names 
mutate(season = as_factor(season)) #make season a factor 
 
```

| Model Number | Season | Distance to forest edge | Model Description |
|------------------|------------------|------------------|-------------------|
| 0 |  |  | no predictors( null model) |
| 1 | X | X | Year + Distance(saturated model) |
| 2 | X |  | Year |
| 3 |  | X | Distance from edge |

: Table of Models

#### e. Run the models (8 points)

```{r Running GLM}

# model 0: null model
model0 <- glm(
  sp ~ 1, # formula
  data = nest_boxes_clean, # data frame
  family = binomial() #binary response variable
)

# model 1: all predictors
model1 <- glm(
  sp ~ season + edge_distance, # formula
  data = nest_boxes_clean, # data frame
  family = binomial() #binary response variable
)

# model 2: season as a predictor
model2 <- glm(
  sp ~ season,
  data = nest_boxes_clean,
  family = binomial()
)

# model 3: Distance from forest edge as a predictor 
model3 <- glm(
  sp ~ edge_distance,
  data = nest_boxes_clean,
  family = binomial() #binary response variable
)

```

#### f. Check the diagnostics (6 points)

Check your diagnostics for all models using simulated residuals from the `DHARMa` package.

Display the diagnostic plots for each model.

```{r Diagnostics}

par(mfrow = c(2, 2)) # Set up the 2x2 plot grid before plotting

# Plot DHARMa residual diagnostics for each model
Model0 <- plot(simulateResiduals(model0)) #for null model
Model1 <- plot(simulateResiduals(model1)) #for saturated model
Model2 <- plot(simulateResiduals(model2)) #for season predictor 
Model3 <- plot(simulateResiduals(model3)) # for distance from forest edge predictor 

```

#### g. Select the best model (6 points)

```{r AIC}
AICc(model1,
     model2,
     model3,
     model0) |>  #calculating AIC for all four models to dtermine best fit 
  arrange(AICc) #setting outputs in ascvending order, to show lowest one first 
```

The best model that predicts Swift Parrot nest box occupancy, as determined by Akaike's Information Criterion (AIC), is the model which includes both season and distance from forest edge as predictors and has the lowest AIC of 226.3133.

#### h. Visualize the model predictions (24 points)

```{r}
# Generate model predictions
model_prediction <- ggpredict(model1, terms = c("edge_distance [all]", "season"))|> # Generate model predictions based on edge_distance and season
rename(season = group) #renaming season as group so I can use it for the other graphs 



nest_boxes_clean <- nest_boxes_clean |> 
  mutate(sp = as.numeric(as.character(sp))) #makes sp numeric 


ggplot(nest_boxes_clean, 
       aes(x = edge_distance, y = sp,  #creating plot off nest_boxes_clean dataset, and setting which data is being plotted. 
           color = season)) +  #setting color by season
  
  
  geom_point(size = 2, #choosing sizes
             alpha = 0.4) + #displaying underlying data 

 
  geom_ribbon(data = model_prediction, 
              aes(x = x, 
                  y = predicted,
                  ymin = conf.low, 
                  ymax = conf.high, 
                  fill = season), #color the ribbons by year 
              alpha = 0.15, #controls transparency 
              inherit.aes = FALSE) + #plotting the 95% CI as ribbons around line 
  
 
  geom_line(data = model_prediction,  # Prediction line
            aes(x = x, y = predicted, color = season),
            size = 1.2, inherit.aes = FALSE) + #making the lines thicker 

  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) + #Customizing y-axis: setting range 0–1 and tick marks at 0, 0.5, and 1
  scale_fill_manual(values = c("2016" = "orange", "2019" = "green")) + #manually coloring ribbons by year
  scale_color_manual(values = c("2016" = "orange", "2019" = "green")) + #manualy coloring points and lines by year


  theme_minimal(base_size = 14) + #changing theme to minimal 
  theme(
    panel.background = element_blank(), #removes gray backround
    panel.grid = element_blank(), #removes grid lines
    axis.ticks = element_line(color = "black"),
    axis.line = element_line(color = "black"),
    legend.position = c(0.9, 0.6) #choosing position of legend 
  ) + #customizing the graph 

  labs(  # Changing labels 
    title = "Probability of Swift Parrot Occupation of Nest Boxes",
    x = "Distance to forest edge (m)",
    y = "Probability of box occupancy(%)"
  )

```

#### i. Write a caption for your figure. (7 points)

***Figure 1: Probability of Swift Parrots occupying nest boxes predicted by distance to forest edge and season*** Data collected by the Australian National University and from the "occdist.csv" package(Stojanovic, Dejan; Owens, Giselle; etc, 2021), DOI: https://datadryad.org/dataset/doi:10.5061/dryad.83bk3j9sb. The points represent the predicted probabilities of nest box occupancy at different values of the predictor "distance to forest edge(m)"(n= 232). The ribbons represent the 95% confidence intervals around model 1's predicted probabilities, our points. The season for 2016 is represented in the orange ribbon, while the season for 2019 is represented by green ribbon.

#### j. Calculate model predictions (4 points)

Calculate the predicted probabilities of Swift Parrot nest box occupancy with 95% at 0 m from forest edge and 900 m from forest edge for each level in `season`.

Show and annotate all code. Display the output.

```{r CAlculating Predictions}

ggpredict(model1, 
          terms = c("edge_distance [0, 900]", "season [2016]")) #calculating the predicted probaility at 0m and 900m for 2016

ggpredict(model1, 
          terms = c("edge_distance [0, 900]", "season [2019]"))  #calculating the predicted probaility at 0m and 900m for 2019


```

#### k. Interpret your results (16 points)

Write 3-5 sentences summarizing what you found, making references to the figure you made in part h and the predictions you calculated in part j. 

Swift Parrots tend to occupy nest boxes more often when the boxes are located closer to the forest edge(Graph in part H).
According the model predictions from part J, if the nest box distance from forest edge(m) is 0m in the season 2016, the probability of a Swift Parrot occupying it is 0.48 (95% CI \[0, 64\], and if the distance is 900m , the probability of a Swift Parrot occupying it is 0.12 (95% CI \[0.06, 24\].If the nest box distance from forest edge(m) is 0m in the season 2019, the probability of a Swift Parrot occupying it is 0.3 (95% CI \[0.18, 0.44\], and if the distance is 900m, the probability of a Swift Parrot occupying it is 0.06 (95% CI \[0.03, 0.13\]. These results suggest that Swift Parrots prefer to be near forest edges, most likely because of proximity to food and nest resources or great competition from Tree Martins, because the Tree martin has a positive relationship between teh varaibles, where box occupancy increases as distance from forest edge increases. 


## Problem 4. Affective and exploratory visualizations (45 points)

#### a. Comparing visualizations (20 points)

Compare and contrast your affective visualization from Homework 3 and the exploratory visualizations you made for Homework 2. In 1-3 sentences each, explain:

-   How are the visualizations different from each other in the way you have represented your data?

In my exploratory visualization for HW-2, I created a boxplot and line graph that only displayed the statistical aspects of my data, which at this point of the quarter, provided data for only one of my groups: School Day. While I focused a lot on graphical data representation for my data on HW-2, my affective visualization focused more on contextualizing the study and indicating certain data themes. Instead of visualizing my data graphically and statistically, I used art to convey different proportions of spoons and water to represent trends I noticed in the data, rather than representing actual data points.

-   What similarities do you see between all your visualizations?

I observe that in both my HW-2 and my affective visualization, I attempted to convey the trends of my data in the context of multiple different variables:time spent cooking(min), number of ingredients used, and Status of day. I did not only focus on displaying the means of my data, but also portray how differently paired variables had unique outcomes outside of time spent cooking. In the end, all of these visualizations displayed the same data and overall theme.

-   What patterns (e.g. differences in means/counts/proportions/medians, trends through time, relationships between variables) do you see in each visualization? Are these different between visualizations? If so, why? If not, why not?

With only four observations at the time of HW-2, my visualization lacked a lot of data, however, I did notice that it's a pattern in all my visualizations to focus on my "time spent cooking(min)" variable. However, in my affective visualization I made a unique pairing of varaibles: Number of dirtied dishes and Number of ingredients, which displays as number of ingredients increases, so does the number of dirty dishes.

-   What kinds of feedback did you get during week 9 in workshop or from the instructors? How did you implement or try those suggestions? If you tried and kept those suggestions, explain how and why; if not, explain why not.

Professor Bui recommended that I add bowls filled with various levels of water to represent the overall trends in my data, how using certain cooking appliances resulted in more time spent cooking, as well as trying to incoporate my "Number of dishes used" variable. At the time I recieved feedback I was still in my sketching out phase, so I built upon her suggestions by symbolizing my time spent cooking in water bowls, as well as symbolized my number of dirty doshes as differently sized spoons that were next to shelves that each contained different numbers of ingredients. I incorporated her recommendations because they gave me a way to incorporate the outcomes of my data, but also have a scenic visualization, and I added beyond the recommendations so that I could include all my measured variables to hopefullt provide even more context to my future audience.
