---
title: "Final"
author: "Alexa Andrade"
format: html
date: 6/08/25
editor: visual
    message: false
    warning: false
    toc: true
    toc-depth: 5
---

### link to Github repo:https://github.com/Alex2002UC/ENVS-193DS_spring-2025_final

## Problem 1. Research writing (36 points)

### Problem

You're working on a research team trying to understand how agricultural runoff in the San Joaquin River Delta influences total nitrogen load (measured in kilograms per year, kg year^-1^). Your co-worker runs some analyses and writes up a report, giving it to you to review. In part 1 of the results section of the report, your co-worker has written:

> We rejected the null hypothesis that there is no correlation between distance from headwater (km) and annual total nitrogen load (kg year^-1^) (p = 0.03).

In part 2 of the results section of the report, your co-worker has written:

> We rejected the null hypothesis that there is no difference in average nitrogen load (kg year^-1^) between sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands) (p = 0.02).

It's great that this report is coming together, but you think they can improve on what they've written and make it more understandable to a non-statistical audience.

#### a. Transparent statistical methods (8 points)

What statistical tests did your co-worker use? Clearly connect the test to the part that you are addressing (e.g. "In part 1, they used \_\_\_\_\_\_\_. In part 2, they used \_\_\_\_\_\_\_.").

In part 1, my co-worker used a Pearson's Correlation Test, to measure the linear relationship between distance to headwater(km) and annual total nitrogen load(kg year-1) In part 2, they used a one-way ANOVA, to test weither the mean nitrogen load would change among different sources: urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands.

#### b. More information needed (8 points)

My coworker should include the outcomes of both the Leven's test and the Shapiro-Wilk normality test. In this case, a one-way ANOVA was used to test whether average nitrogen load (kg year⁻¹) differs between sources: urban land, atmospheric deposition, fertilizer, waste water treatment, and grasslands. However, ANOVA relies on two key assumptions: normality of residuals and homogeneity of variances.

## Alternative hypothesis (H A): At least two means are not equal (as in, just two could be different, or all of them could be different)


The Shapiro-Wilk normality test should be used to check whether the residuals from the model are approximately normally distributed. This is crucial because the validity of the ANOVA results depends on this assumption. We would ideally want all of our source varaibles to have a test statistic near 1, as that would mean the samples do fit the standarized normal. However, say that the resulting test-staitsic from the Shapiro-Wilk normality test was W=0.7 for fertilizer, then this would indicate that the residuals are not normally distribted, and that a one-way Anova may not be appropriatate to test the null hypothesis.

To confirm that there is homogeniety across the variances of urban land, atmospheric deposition, fertilizer, waste water treatment, and grasslands. Levene's test compares the variance of nitrogen load(kg year⁻¹) among the different sources. If the output of the test is p \< 0.05, we would reject the null hypothesis that there is equal varaiances between groups, however we wouldn't know which group comaparison doesn't have equal variance. The outcome could determine if we need more tests, or if a one-way ANOVA would not be appropraite and a different test needs to be used.

These two additional statistics would confirm whether the one-way ANOVA was appropriate and whether the results can be trusted.

## Maybe:

Post-hoc pairwise comparisons (e.g., Tukey’s HSD) – Since the ANOVA only indicates that at least one group differs, follow-up tests are needed to identify which specific sources (e.g., fertilizer vs. urban land) have significantly different nitrogen loads.

Effect size (e.g., η² or Cohen’s f) – The p-value only tells us that a difference exists, but not how large the difference is. Reporting effect size would help quantify the practical significance of the variation in nitrogen load across sources.

#### c. Suggestions for rewriting (20 points)

In 1-3 sentences, *write new research statements* to include relevant components from parts a-b and a full test summary in parentheses to be transparent about the statistical method.

Be sure that your rewritten statements clearly delineate the *biological narrative* from the *statistical summary*. See lecture and workshop for examples of writing.

Note that your co-worker didn't include any information about the test statistic, distribution, etc., and that you only know the p-value. For any part that you do not know, list that part with text. For example, you could write something like: "... **r = correlation coefficient**, p = 0.03, $\alpha$ = significance level ..."

> We rejected the null hypothesis that there is no correlation between distance from headwater (km) and annual total nitrogen load (kg year^-1^) (p = 0.03).

We found a statistically significant correlation between distance from headwater(km) and annual total nitrogen load(kg year^-1^) (Pearson’s r = correlation coefficient, p = 0.03, α = significance level]), suggesting that nitrogen load changes with distance downstream.


In part 2 of the results section of the report, your co-worker has written:

> We rejected the null hypothesis that there is no difference in average nitrogen load (kg year^-1^) between sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands) (p = 0.02).

We found that there is a difference between average nitrogen load (kg year^-1^) between sources: urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands(one-way ANOVA, F(among groups df, within groups df) = F-statistic, p = 0.02, $\alpha = significance level).



## Problem 2. Data visualization (36 points)

### Skills you will demonstrate

In this problem, you will demonstrate your ability to find, clean, summarize, and visualize a data set. You will use data from the Santa Barbara Coastal LTER on sea surface temperature in the Santa Barbara Channel.

Note that the instructions do not include all the steps you need to do *on purpose*. You will need to use your skills in working backwards from a final product (the visualization given to you) to determine the steps to get there.

### Problem

Navigate to the [SBC LTER's data catalog](https://sbclter.msi.ucsb.edu/data/catalog/). Find and download the dataset on sea surface temperature in the Santa Barbara Channel to your `data` folder. (Hint: the page on the data catalog is called "Sea Surface Temperature in SB channel".)

In your set up chunk, read in the data as an object called `sst`.

#### a. Cleaning and summarizing (16 points)

Create an object called `sst_clean` from `sst`. Clean and summarize the data such that you end up with a data frame like this:

![](/assignments/images/final/slice-sample-example.png)

with a structure like this:

![](/assignments/images/final/str-example.png)

Use the pipe operator (`|>` or `%>%`) to string functions together. After every pipe, start a new line to use the next function. Include annotations for each function you use.

When you are done with all your cleaning steps, display 5 rows from `sst_clean` using `slice_sample()` and the structure using `str()`.

Show all code.

::: {.callout-tip title="Consider using the `lubridate` package." collapse="true"}
You may find it useful to use functions from the `lubridate` package (pre-loaded in `tidyverse`, so you don't have to install/read it in separately).

Not sure where to start? Google the package and read about it.
:::

::: {.callout-tip title="Double check your code!" collapse="true"}
When writing your cleaning and summarizing code, run your code after adding each function. Does your data frame look the way you'd expect?

Additionally, **pay attention to the components in the figure.** What has to be true about your data frame in order for you to recreate this figure? (Hint: think about what years are displayed in the figure, and contrast that with the years in the data set.)
:::

#### b. Visualize the data (20 points)

Recreate this visualization:

![](/assignments/images/final/SST.jpeg)

In addition to displaying the correct subset of the data, the specific aesthetic components you need to recreate are:

-   the geometries (there are two)
-   the x-, y-axis, and legend text and labels
-   a color gradient in a single color (doesn't have to be blue, can be any other color) going from light --\> dark for 2018 --\> 2023
-   the legend position inside the panel
-   the panel border and background

## Problem 3. Data analysis (87 points)

### Skills you will demonstrate

In this problem, you will demonstrate your ability to **understand a data set that someone else collected** and **identify and execute the appropriate statistical method, with all assumption checks**. Additionally, you will demonstrate your ability to **visualize and communicate about the results of your statistical test**.

You will be working with the nest box occupancy dataset from Stojanovic, D., Owens, G., Young, C.M., Alves, F. and Heinsohn, R. 2021. "Do nest boxes breed the target species or its competitors? A case study of a critically endangered bird." *Restoration Ecology*. DOI: [10.1111/rec.13319](https://doi.org/10.1111/rec.13319)

The data citation is: Stojanovic, Dejan et al. (2021). Do nest boxes breed the target species or its competitors? A case study of a critically endangered bird \[Dataset\]. Dryad. <https://doi.org/10.5061/dryad.83bk3j9sb>

You will answer the following research questions:

1.  How do year (2016 or 2019) and distance from forest edge predict Swift Parrot (*Lathamus discolor*) nest box occupancy?
2.  Is there a simpler model that explains Swift Parrot nest box occupancy, and if so, what is it?

**READ THE INTRODUCTION AND METHODS BEFORE YOU START.**

### Problem

::: {.callout-tip title="Before doing the following parts..." collapse="true"}
Read in your data in the set up chunk, storing it as a new object called `nest_boxes`.

Do any exploring you need to do (exploratory visualizations, etc.) but **DO NOT include any code or output from your data exploration.**

Note that you may have some cleaning/wrangling steps to do. Do them before running your models!

Not sure how to write code but hide it and/or its output from the final rendered document? Check [here](https://quarto.org/docs/computations/execution-options.html) for options.
:::

#### a. Response variable (2 points)

In 1-2 sentences, explain what the 1s and 0s mean in this data set biologically.

#### b. Purpose of study (2 points)

The authors compare nest box occupancy between 3 species: Swift Parrots, Common Starlings, and Tree Martins. In 1-2 sentences, explain the main difference between Swift Parrots and the other two species in the context of this study.

#### c. Difference in "seasons" (2 points)

The authors compare two years (that they refer to as "seasons"). In 1-2 sentences, define what those years/seasons are, and explain how they differ in the context of this study.

#### d. Table of models (10 points)

Make a table of all the models you will need to run. You will run 4 models: a null model, a saturated model, and two other models with different combinations of predictors.

Stuck on how to create a table? See workshop 8 for an example.

Your table should have 4 columns: (1) model number, (2) season, (3) distance to forest edge, and (4) model description.

#### e. Run the models (8 points)

Write your code to run all your models. Do not display any output.

#### f. Check the diagnostics (6 points)

Check your diagnostics for all models using simulated residuals from the `DHARMa` package.

Display the diagnostic plots for each model.

#### g. Select the best model (6 points)

Using Akaike's Information Criterion (AIC) from the `MuMIn` package, choose the best model.

In text, write what the best model was (i.e. "The best model as determined by Akaike's Information Criterion (AIC)...").

**Use the predictors and the response variable to describe the model**, not the model number that you assigned.

#### h. Visualize the model predictions (24 points)

Create a plot showing model predictions with 95% confidence intervals and the underlying data.

Show and annotate all code. Show the output.

For full credit:

-   make sure the x- and y-axis labels are written in full
-   take out the gridlines
-   use colors that are different from the default

#### i. Write a caption for your figure. (7 points)

Include a figure number, title, description of the figure, and data citation.

#### j. Calculate model predictions (4 points)

Calculate the predicted probabilities of Swift Parrot nest box occupancy with 95% at 0 m from forest edge and 900 m from forest edge for each level in `season`.

Show and annotate all code. Display the output.

#### k. Interpret your results (16 points)

Write 3-5 sentences summarizing what you found, making references to the figure you made in part h and the predictions you calculated in part j. Your summary should include your interpretation of:

-   the predicted probability of occupancy at the forest edge (0 m) and farther away from the forest edge (900 m) between seasons
-   the relationship between distance from forest edge and probability of occupancy
-   the biology behind the trends you found - what explains the relationship between distance from forest edge and probability of Swift Parrot nest box occupancy?

::: {.callout-note title="READ THE PAPER!" collapse="true"}
To understand the biology behind the trends you found, you need to read the paper and look at the figures. For which species do you see the *opposite* relationship between distance to forest edge and probability of occupancy?
:::

## Problem 4. Affective and exploratory visualizations (45 points)

### Skills you will demonstrate

In this problem, you will demonstrate your ability to **communicate about your visualization and give feedback to others.** You will also demonstrate your ability to **design and execute an appropriate statistical analysis for your data**.

### Problem 4

#### a. Comparing visualizations (20 points)

Compare and contrast your affective visualization from Homework 3 and the exploratory visualizations you made for Homework 2. In 1-3 sentences each, explain:

-   How are the visualizations different from each other in the way you have represented your data?

In my exploratory visualization for HW-2, I created a boxplot and line graph that only displayed the statistical aspects of my data, which at this point of the quarter, provided data for only one of my groups: School Day. While I focused a lot on graphical data representation for my data on HW-2, my affective visualization focused more on contextualizing the study and indicating certain data themes. Instead of visualizing my data graphically and statistically, I used art to convey different proportions of spoons and water to represent trends I noticed in the data, rather than representing actual data points. 

-   What similarities do you see between all your visualizations?

I observe that in both my HW-2 and my affective visualization, I attempted to convey the trends of my data in the context of multiple different variables:time spent cooking(min), number of ingredients used, and Status of day. I did not only focus on displaying the means of my data, but also portray how differently paired variables had unique outcomes outside of time spent cooking. In the end, all of these visualizations displayed the same data and overall theme. 

-   What patterns (e.g. differences in means/counts/proportions/medians, trends through time, relationships between variables) do you see in each visualization? Are these different between visualizations? If so, why? If not, why not?

With only four observations at the time of HW-2, my visualization lacked a lot of data, however, I did notice that it's a pattern in all my visualizations to focus on my "time spent cooking(min)" variable. However, in my affective visualization I made a unique pairing of varaibles: Number of dirtied dishes and Number of ingredients, which displays as number of ingredients increases, so does the number of dirty dishes. 

-   What kinds of feedback did you get during week 9 in workshop or from the instructors? How did you implement or try those suggestions? If you tried and kept those suggestions, explain how and why; if not, explain why not.

Professor Bui recommended that I add bowls filled with various levels of water to represent the overall trends in my data, how using certain cooking appliances resulted in more time spent cooking, as well as trying to incoporate my "Number of dishes used" variable. At the time I recieved feedback I was still in my sketching out phase, so I built upon her suggestions by symbolizing my time spent cooking in water bowls, as well as symbolized my number of dirty doshes as differently sized spoons that were next to shelves that each contained different numbers of ingredients. I incorporated her recommendations because they gave me a way to incorporate the outcomes of my data, but also have a scenic visualization, and I added beyond the recommendations so that I could include all my measured variables to hopefullt provide even more context to my future audience. 


#### b. Sharing your affective visualization (25 points)

This is a component you will complete in workshop during week 10. **We will be taking attendance that day. If you attend class and complete the activity, you will receive full credit for this section.**
